{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNiWam180Jkh",
        "outputId": "f0bb83d4-5b6f-4dd9-84bc-61c790bbf5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7121/7121 [==============================] - 104s 14ms/step - ae_loss: 0.1067 - d_loss: 9.5056e-04\n",
            "Epoch 2/5\n",
            "7121/7121 [==============================] - 98s 14ms/step - ae_loss: 0.1027 - d_loss: 2.0726e-04\n",
            "Epoch 3/5\n",
            "7121/7121 [==============================] - 93s 13ms/step - ae_loss: 0.1027 - d_loss: 0.0041\n",
            "Epoch 4/5\n",
            "7121/7121 [==============================] - 97s 14ms/step - ae_loss: 0.1027 - d_loss: 0.0020\n",
            "Epoch 5/5\n",
            "7121/7121 [==============================] - 96s 14ms/step - ae_loss: 0.1027 - d_loss: 0.0020\n",
            "1781/1781 [==============================] - 11s 6ms/step\n",
            "Precision-Recall AUC: 0.1625730737170408\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95     56864\n",
            "           1       0.02      0.92      0.03        98\n",
            "\n",
            "    accuracy                           0.90     56962\n",
            "   macro avg       0.51      0.91      0.49     56962\n",
            "weighted avg       1.00      0.90      0.95     56962\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/creditcard.csv')\n",
        "df = df.dropna()\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(df.drop('Class', axis=1))\n",
        "features = features_scaled\n",
        "labels = df['Class'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def geometric_mask(data, mask_rate=0.1):\n",
        "    mask = np.random.geometric(p=mask_rate, size=data.shape)\n",
        "    return np.where(mask > 1, 0, data)\n",
        "\n",
        "x_train_augmented = geometric_mask(x_train)\n",
        "\n",
        "\n",
        "def transformer_encoder(inputs):\n",
        "    inputs = tf.expand_dims(inputs, -1)\n",
        "    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=inputs.shape[-1], dropout=0.2)(inputs, inputs)\n",
        "    attention_output = layers.Dropout(0.2)(attention_output)\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
        "    ffn_output = layers.Dense(units=inputs.shape[-1] * 2, activation='relu')(attention_output)\n",
        "    ffn_output = layers.Dropout(0.2)(ffn_output)\n",
        "    ffn_output = layers.Dense(units=inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = layers.LayerNormalization(epsilon=1e-6)(ffn_output + attention_output)\n",
        "    return tf.squeeze(ffn_output, axis=-1)\n",
        "\n",
        "\n",
        "input_layer = layers.Input(shape=(x_train.shape[1],))\n",
        "encoded = transformer_encoder(input_layer)\n",
        "decoded = layers.Dense(x_train.shape[1], activation='sigmoid')(encoded)\n",
        "autoencoder = keras.Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "class ContrastiveDiscriminator(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ContrastiveDiscriminator, self).__init__()\n",
        "        self.discriminator_layers = keras.Sequential([\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.discriminator_layers(inputs, training=training)\n",
        "\n",
        "    def contrastive_loss(self, real, fake, temperature=0.1):\n",
        "        if len(real.shape) > 2:\n",
        "            real = tf.reshape(real, [tf.shape(real)[0], -1])\n",
        "        if len(fake.shape) > 2:\n",
        "            fake = tf.reshape(fake, [tf.shape(fake)[0], -1])\n",
        "\n",
        "        real_norm = tf.math.l2_normalize(real, axis=1)\n",
        "        fake_norm = tf.math.l2_normalize(fake, axis=1)\n",
        "\n",
        "        real_norm = tf.cast(real_norm, tf.float32)\n",
        "        fake_norm = tf.cast(fake_norm, tf.float32)\n",
        "\n",
        "        dot_product = tf.matmul(real_norm, fake_norm, transpose_b=True)\n",
        "        exp_product = tf.exp(dot_product / temperature)\n",
        "\n",
        "        diagonal = tf.linalg.diag_part(exp_product)\n",
        "        loss = -tf.math.log(tf.reduce_sum(diagonal) / tf.reduce_sum(exp_product))\n",
        "        return loss\n",
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, autoencoder, discriminator):\n",
        "        super(GAN, self).__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, ae_opt, d_opt, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.ae_opt = ae_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_data = data\n",
        "\n",
        "        with tf.GradientTape() as ae_tape, tf.GradientTape() as d_tape:\n",
        "            reconstruction = self.autoencoder(real_data, training=True)\n",
        "\n",
        "            ae_loss = self.loss_fn(real_data, reconstruction)\n",
        "\n",
        "            real_output = self.discriminator(real_data, training=True)\n",
        "            fake_output = self.discriminator(reconstruction, training=True)\n",
        "\n",
        "            d_loss_real = self.loss_fn(tf.ones_like(real_output), real_output)\n",
        "            d_loss_fake = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "        ae_grads = ae_tape.gradient(ae_loss, self.autoencoder.trainable_variables)\n",
        "        d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.ae_opt.apply_gradients(zip(ae_grads, self.autoencoder.trainable_variables))\n",
        "        self.d_opt.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
        "\n",
        "        return {\"ae_loss\": ae_loss, \"d_loss\": d_loss}\n",
        "\n",
        "\n",
        "discriminator = ContrastiveDiscriminator()\n",
        "gan = GAN(autoencoder, discriminator)\n",
        "gan.compile(\n",
        "    ae_opt=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    d_opt=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss_fn=keras.losses.MeanSquaredError()\n",
        ")\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train_augmented).batch(32)\n",
        "gan.fit(train_dataset, epochs=50)\n",
        "\n",
        "\n",
        "predictions = autoencoder.predict(x_test)\n",
        "mse = np.mean(np.power(x_test - predictions, 2), axis=1)\n",
        "threshold = np.percentile(mse, 90)  # Lowering the threshold to catch fewer, more likely anomalies\n",
        "anomalies = mse > threshold\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, mse)\n",
        "auc_score = auc(recall, precision)\n",
        "\n",
        "\n",
        "print(f\"Precision-Recall AUC: {auc_score}\")\n",
        "print(classification_report(y_test, anomalies))\n"
      ]
    }
  ]
}